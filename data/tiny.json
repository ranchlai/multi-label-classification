[
 {
  "sentence": "Recent advances in deep learning have enabled the development of automated\nframeworks for analysing medical images and signals, including analysis of\ncervical cancer. Many previous works focus on the analysis of isolated cervical\ncells, or do not offer sufficient methods to explain and understand how the\nproposed models reach their classification decisions on multi-cell images.\nHere, we evaluate various state-of-the-art deep learning models and\nattention-based frameworks for the classification of images of multiple\ncervical cells. As we aim to provide interpretable deep learning models to\naddress this task, we also compare their explainability through the\nvisualization of their gradients. We demonstrate the importance of using images\nthat contain multiple cells over using isolated single-cell images. We show the\neffectiveness of the residual channel attention model for extracting important\nfeatures from a group of cells, and demonstrate this model's efficiency for\nthis classification task. This work highlights the benefits of channel\nattention mechanisms in analyzing multiple-cell images for potential relations\nand distributions within a group of cells. It also provides interpretable\nmodels to address the classification of cervical cells.",
  "label": [
   "cs.CV",
   "cs.LG"
  ]
 },
 {
  "sentence": "Methods for object detection and segmentation rely on large scale\ninstance-level annotations for training, which are difficult and time-consuming\nto collect. Efforts to alleviate this look at varying degrees and quality of\nsupervision. Weakly-supervised approaches draw on image-level labels to build\ndetectors/segmentors, while zero/few-shot methods assume abundant\ninstance-level data for a set of base classes, and none to a few examples for\nnovel classes. This taxonomy has largely siloed algorithmic designs. In this\nwork, we aim to bridge this divide by proposing an intuitive and unified\nsemi-supervised model that is applicable to a range of supervision: from zero\nto a few instance-level samples per novel class. For base classes, our model\nlearns a mapping from weakly-supervised to fully-supervised\ndetectors/segmentors. By learning and leveraging visual and lingual\nsimilarities between the novel and base classes, we transfer those mappings to\nobtain detectors/segmentors for novel classes; refining them with a few novel\nclass instance-level annotated samples, if available. The overall model is\nend-to-end trainable and highly flexible. Through extensive experiments on\nMS-COCO and Pascal VOC benchmark datasets we show improved performance in a\nvariety of settings.",
  "label": [
   "cs.CV"
  ]
 },
 {
  "sentence": "Video summarization aims to facilitate large-scale video browsing by\nproducing short, concise summaries that are diverse and representative of\noriginal videos. In this paper, we formulate video summarization as a\nsequential decision-making process and develop a deep summarization network\n(DSN) to summarize videos. DSN predicts for each video frame a probability,\nwhich indicates how likely a frame is selected, and then takes actions based on\nthe probability distributions to select frames, forming video summaries. To\ntrain our DSN, we propose an end-to-end, reinforcement learning-based\nframework, where we design a novel reward function that jointly accounts for\ndiversity and representativeness of generated summaries and does not rely on\nlabels or user interactions at all. During training, the reward function judges\nhow diverse and representative the generated summaries are, while DSN strives\nfor earning higher rewards by learning to produce more diverse and more\nrepresentative summaries. Since labels are not required, our method can be\nfully unsupervised. Extensive experiments on two benchmark datasets show that\nour unsupervised method not only outperforms other state-of-the-art\nunsupervised methods, but also is comparable to or even superior than most of\npublished supervised approaches.",
  "label": [
   "cs.CV"
  ]
 },
 {
  "sentence": "In this paper, we introduce a collaborative training algorithm of balanced\nrandom forests with convolutional neural networks for domain adaptation tasks.\nIn real scenarios, most domain adaptation algorithms face the challenges from\nnoisy, insufficient training data and open set categorization. In such cases,\nconventional methods suffer from overfitting and fail to successfully transfer\nthe knowledge of the source to the target domain. To address these issues, the\nfollowing two techniques are proposed. First, we introduce the optimized\ndecision tree construction method with convolutional neural networks, in which\nthe data at each node are split into equal sizes while maximizing the\ninformation gain. It generates balanced decision trees on deep features because\nof the even-split constraint, which contributes to enhanced discrimination\npower and reduced overfitting problem. Second, to tackle the domain\nmisalignment problem, we propose the domain alignment loss which penalizes\nuneven splits of the source and target domain data. By collaboratively\noptimizing the information gain of the labeled source data as well as the\nentropy of unlabeled target data distributions, the proposed CoBRF algorithm\nachieves significantly better performance than the state-of-the-art methods.",
  "label": [
   "cs.CV"
  ]
 },
 {
  "sentence": "Policy gradient methods are among the most effective methods in challenging\nreinforcement learning problems with large state and/or action spaces. However,\nlittle is known about even their most basic theoretical convergence properties,\nincluding: if and how fast they converge to a globally optimal solution or how\nthey cope with approximation error due to using a restricted class of\nparametric policies. This work provides provable characterizations of the\ncomputational, approximation, and sample size properties of policy gradient\nmethods in the context of discounted Markov Decision Processes (MDPs). We focus\non both: \"tabular\" policy parameterizations, where the optimal policy is\ncontained in the class and where we show global convergence to the optimal\npolicy; and parametric policy classes (considering both log-linear and neural\npolicy classes), which may not contain the optimal policy and where we provide\nagnostic learning results. One central contribution of this work is in\nproviding approximation guarantees that are average case -- which avoid\nexplicit worst-case dependencies on the size of state space -- by making a\nformal connection to supervised learning under distribution shift. This\ncharacterization shows an important interplay between estimation error,\napproximation error, and exploration (as characterized through a precisely\ndefined condition number).",
  "label": [
   "cs.LG"
  ]
 },
 {
  "sentence": "Camera-equipped drones can capture targets on the ground from a wider field\nof view than static cameras or moving sensors over the ground. In this paper we\npresent a large-scale vehicle detection and counting benchmark, named\nDroneVehicle, aiming at advancing visual analysis tasks on the drone platform.\nThe images in the benchmark were captured over various urban areas, which\ninclude different types of urban roads, residential areas, parking lots,\nhighways, etc., from day to night. Specifically, DroneVehicle consists of\n15,532 pairs of images, i.e., RGB images and infrared images with rich\nannotations, including oriented object bounding boxes, object categories, etc.\nWith intensive amount of effort, our benchmark has 441,642 annotated instances\nin 31,064 images. As a large-scale dataset with both RGB and thermal infrared\n(RGBT) images, the benchmark enables extensive evaluation and investigation of\nvisual analysis algorithms on the drone platform. In particular, we design two\npopular tasks with the benchmark, including object detection and object\ncounting. All these tasks are extremely challenging in the proposed dataset due\nto factors such as illumination, occlusion, and scale variations. We hope the\nbenchmark largely boost the research and development in visual analysis on\ndrone platforms. The DroneVehicle dataset can be download from\nhttps://github.com/VisDrone/DroneVehicle.",
  "label": [
   "cs.CV",
   "cs.LG"
  ]
 },
 {
  "sentence": "Introduction: Real-world data generated from clinical practice can be used to\nanalyze the real-world evidence (RWE) of COVID-19 pharmacotherapy and validate\nthe results of randomized clinical trials (RCTs). Machine learning (ML) methods\nare being used in RWE and are promising tools for precision-medicine. In this\nstudy, ML methods are applied to study the efficacy of therapies on COVID-19\nhospital admissions in the Valencian Region in Spain. Methods: 5244 and 1312\nCOVID-19 hospital admissions - dated between January 2020 and January 2021 from\n10 health departments, were used respectively for training and validation of\nseparate treatment-effect models (TE-ML) for remdesivir, corticosteroids,\ntocilizumab, lopinavir-ritonavir, azithromycin and\nchloroquine/hydroxychloroquine. 2390 admissions from 2 additional health\ndepartments were reserved as an independent test to analyze retrospectively the\nsurvival benefits of therapies in the population selected by the TE-ML models\nusing cox-proportional hazard models. TE-ML models were adjusted using\ntreatment propensity scores to control for pre-treatment confounding variables\nassociated to outcome and further evaluated for futility. ML architecture was\nbased on boosted decision-trees. Results: In the populations identified by the\nTE-ML models, only Remdesivir and Tocilizumab were significantly associated\nwith an increase in survival time, with hazard ratios of 0.41 (P = 0.04) and\n0.21 (P = 0.001), respectively. No survival benefits from chloroquine\nderivatives, lopinavir-ritonavir and azithromycin were demonstrated. Tools to\nexplain the predictions of TE-ML models are explored at patient-level as\npotential tools for personalized decision making and precision medicine.\nConclusion: ML methods are suitable tools toward RWE analysis of COVID-19\npharmacotherapies. Results obtained reproduce published results on RWE and\nvalidate the results from RCTs.",
  "label": [
   "cs.LG"
  ]
 },
 {
  "sentence": "Facial expressions of emotion are a major channel in our daily\ncommunications, and it has been subject of intense research in recent years. To\nautomatically infer facial expressions, convolutional neural network based\napproaches has become widely adopted due to their proven applicability to\nFacial Expression Recognition (FER) task.On the other hand Virtual Reality (VR)\nhas gained popularity as an immersive multimedia platform, where FER can\nprovide enriched media experiences. However, recognizing facial expression\nwhile wearing a head-mounted VR headset is a challenging task due to the upper\nhalf of the face being completely occluded. In this paper we attempt to\novercome these issues and focus on facial expression recognition in presence of\na severe occlusion where the user is wearing a head-mounted display in a VR\nsetting. We propose a geometric model to simulate occlusion resulting from a\nSamsung Gear VR headset that can be applied to existing FER datasets. Then, we\nadopt a transfer learning approach, starting from two pretrained networks,\nnamely VGG and ResNet. We further fine-tune the networks on FER+ and RAF-DB\ndatasets. Experimental results show that our approach achieves comparable\nresults to existing methods while training on three modified benchmark datasets\nthat adhere to realistic occlusion resulting from wearing a commodity VR\nheadset. Code for this paper is available at:\nhttps://github.com/bita-github/MRP-FER",
  "label": [
   "cs.CV"
  ]
 },
 {
  "sentence": "To boost the object grabbing capability of underwater robots for open-sea\nfarming, we propose a new dataset (UDD) consisting of three categories\n(seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our\nknowledge, it is the first 4K HD dataset collected in a real open-sea farm. We\nalso propose a novel Poisson-blending Generative Adversarial Network (Poisson\nGAN) and an efficient object detection network (AquaNet) to address two common\nissues within related datasets: the class-imbalance problem and the problem of\nmass small object, respectively. Specifically, Poisson GAN combines Poisson\nblending into its generator and employs a new loss called Dual Restriction loss\n(DR loss), which supervises both implicit space features and image-level\nfeatures during training to generate more realistic images. By utilizing\nPoisson GAN, objects of minority class like seacucumber or scallop could be\nadded into an image naturally and annotated automatically, which could increase\nthe loss of minority classes during training detectors to eliminate the\nclass-imbalance problem; AquaNet is a high-efficiency detector to address the\nproblem of detecting mass small objects from cloudy underwater pictures. Within\nit, we design two efficient components: a depth-wise-convolution-based\nMulti-scale Contextual Features Fusion (MFF) block and a Multi-scale\nBlursampling (MBP) module to reduce the parameters of the network to 1.3\nmillion. Both two components could provide multi-scale features of small\nobjects under a short backbone configuration without any loss of accuracy. In\naddition, we construct a large-scale augmented dataset (AUDD) and a\npre-training dataset via Poisson GAN from UDD. Extensive experiments show the\neffectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training\ndataset.",
  "label": [
   "cs.CV"
  ]
 },
 {
  "sentence": "Landscapes are meaningful ecological units that strongly depend on the\nenvironmental conditions. Such dependencies between landscapes and the\nenvironment have been noted since the beginning of Earth sciences and cast into\nconceptual models describing the interdependencies of climate, geology,\nvegetation and geomorphology. Here, we ask whether landscapes, as seen from\nspace, can be statistically predicted from pertinent environmental conditions.\nTo this end we adapted a deep learning generative model in order to establish\nthe relationship between the environmental conditions and the view of\nlandscapes from the Sentinel-2 satellite. We trained a conditional generative\nadversarial network to generate multispectral imagery given a set of climatic,\nterrain and anthropogenic predictors. The generated imagery of the landscapes\nshare many characteristics with the real one. Results based on landscape patch\nmetrics, indicative of landscape composition and structure, show that the\nproposed generative model creates landscapes that are more similar to the\ntargets than the baseline models while overall reflectance and vegetation cover\nare predicted better. We demonstrate that for many purposes the generated\nlandscapes behave as real with immediate application for global change studies.\nWe envision the application of machine learning as a tool to forecast the\neffects of climate change on the spatial features of landscapes, while we\nassess its limitations and breaking points.",
  "label": [
   "cs.CV",
   "cs.LG"
  ]
 },
 {
  "sentence": "This paper describes the results of formally evaluating the MCV (Markov\nconcurrent vision) image labeling algorithm which is a (semi-) hierarchical\nalgorithm commencing with a partition made up of single pixel regions and\nmerging regions or subsets of regions using a Markov random field (MRF) image\nmodel. It is an example of a general approach to computer vision called\nconcurrent vision in which the operations of image segmentation and image\nclassification are carried out concurrently. While many image labeling\nalgorithms output a single partition, or segmentation, the MCV algorithm\noutputs a sequence of partitions and this more elaborate structure may provide\ninformation that is valuable for higher level vision systems. With certain\ntypes of MRF the component of the system for image evaluation can be\nimplemented as a hardwired feed forward neural network. While being applicable\nto images (i.e. 2D signals), the algorithm is equally applicable to 1D signals\n(e.g. speech) or 3D signals (e.g. video sequences) (though its performance in\nsuch domains remains to be tested). The algorithm is assessed using subjective\nand objective criteria with very good results.",
  "label": [
   "cs.CV"
  ]
 },
 {
  "sentence": "This paper has been withdrawn by the authors due to insufficient or\ndefinition error(s) in the ethics approval protocol.\n  Autism spectrum disorders (ASD) impact the cognitive, social, communicative\nand behavioral abilities of an individual. The development of new clinical\ndecision support systems is of importance in reducing the delay between\npresentation of symptoms and an accurate diagnosis. In this work, we contribute\na new database consisting of video clips of typical (normal) and atypical (such\nas hand flapping, spinning or rocking) behaviors, displayed in natural\nsettings, which have been collected from the YouTube video website. We propose\na preliminary non-intrusive approach based on skeleton keypoint identification\nusing pretrained deep neural networks on human body video clips to extract\nfeatures and perform body movement analysis that differentiates typical and\natypical behaviors of children. Experimental results on the newly contributed\ndatabase show that our platform performs best with decision tree as the\nclassifier when compared to other popular methodologies and offers a baseline\nagainst which alternate approaches may developed and tested.",
  "label": [
   "cs.CV"
  ]
 },
 {
  "sentence": "We consider graph representation learning in a self-supervised manner. Graph\nneural networks (GNNs) use neighborhood aggregation as a core component that\nresults in feature smoothing among nodes in proximity. While successful in\nvarious prediction tasks, such a paradigm falls short of capturing nodes'\nsimilarities over a long distance, which proves to be important for\nhigh-quality learning. To tackle this problem, we strengthen the graph with two\nadditional graph views, in which nodes are directly linked to those with the\nmost similar features or local structures. Not restricted by connectivity in\nthe original graph, the generated views allow the model to enhance its\nexpressive power with new and complementary perspectives from which to look at\nthe relationship between nodes. Following a contrastive learning approach, we\npropose a method that aims to maximize the agreement between representations\nacross generated views and the original graph. We also propose a channel-level\ncontrast approach that greatly reduces computation cost, compared to the\ncommonly used node level contrast, which requires computation cost quadratic in\nthe number of nodes. Extensive experiments on seven assortative graphs and four\ndisassortative graphs demonstrate the effectiveness of our approach.",
  "label": [
   "cs.LG"
  ]
 },
 {
  "sentence": "CNN is a powerful tool for many computer vision tasks, achieving much better\nresult than traditional methods. Since CNN has a very large capacity, training\nsuch a neural network often requires many data, but it is often expensive to\nobtain labeled images in real practice, especially for object detection, where\ncollecting bounding box of every object in training set requires many human\nefforts. This is the case in detection of retail products where there can be\nmany different categories. In this paper, we focus on applying CNN to detect\n324-categories products in situ, while requiring no extra effort of labeling\nbounding box for any image. Our approach is based on an algorithm that extracts\nbounding box from in-vitro dataset and an algorithm to simulate occlusion. We\nhave successfully shown the effectiveness and usefulness of our methods to\nbuild up a Faster RCNN detection model. Similar idea is also applicable in\nother scenarios.",
  "label": [
   "cs.CV"
  ]
 },
 {
  "sentence": "An important problem in multiview representation learning is finding the\noptimal combination of views with respect to the specific task at hand. To this\nend, we introduce NAM: a Neural Attentive Multiview machine that learns\nmultiview item representations and similarity by employing a novel attention\nmechanism. NAM harnesses multiple information sources and automatically\nquantifies their relevancy with respect to a supervised task. Finally, a very\npractical advantage of NAM is its robustness to the case of dataset with\nmissing views. We demonstrate the effectiveness of NAM for the task of movies\nand app recommendations. Our evaluations indicate that NAM outperforms single\nview models as well as alternative multiview methods on item recommendations\ntasks, including cold-start scenarios.",
  "label": [
   "cs.LG",
   "cs.IR"
  ]
 }
]
